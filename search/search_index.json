{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MCGAE Tutorial","text":"MCGAE is a novel deep computational framework designed for comprehensive analysis of spatial transcriptomics data across platforms, integrating morphological images. Given spatial multi-modal transcriptomics data, MCGAE commences by initially acquiring the original gene expression matrix alongside the adjacency matrix, which is computed based on spatial coordinates. The framework's multi-view construction is facilitated through a modular modelling approach, granting users the flexibility to select from a variety of enhancement methods, including but not limited to simple autoencoders, for the purpose of obtaining enhanced views of gene expression matrix \\(X\\). Furthermore, the construction of multiple views of adjacency matrix \\(A\\) is achievable by employing diverse similarity metrics, thereby enriching the data analysis spectrum by capturing a multitude of perspectives and relationships inherent within the spatial transcriptomics data.       In the ensuing phase, modality-specific spots representations are obtained through contrastive graph convolutional neural networks coupled with attention modules. Specifically, MCGAE keeps \\(X\\) fixed and uses it with different views of \\(A\\), where each pair \\((X_1, A_j)\\) is processed through a Graph Convolutional Network (GCN) to extract multi-view representations that are specifically pertinent to \\(X_1\\). These representations are then fused into a comprehensive embedding \\(Z^X\\) using an attention mechanism, which is utilized for the reconstruction of the original \\(X_1\\). Similarly, by keeping \\(A_1\\) constant and varying \\(X_i\\), MCGAE follows the same process to garner view-specific representations for \\(A_1\\), which are aggregated into \\(Z^A\\). The pair \\((X_1, A_1)\\) is designated as the base graph, encapsulating the original expression data. The refinement of their biological representations is further achieved through the adoption of self-supervised contrastive learning. In instances where morphological images are accessible, MCGAE leverages a pre-trained ResNet50 for the extraction of image features, resulting in the image embedding \\(Z^{\\text{morph}}\\), thereby enhancing the model's capability in processing multimodal data.       During the terminal fusion phase, MCGAE employs an attention mechanism to combine \\(Z^X\\), \\(Z^A\\), and \\(Z^{\\text{morph}}\\), creating the ultimate composite embedding \\(Z\\). This embedding is further refined through an unsupervised deep iterative clustering strategy to enhance its compactness, which is then applied to downstream analytical tasks such as spatial domain identification, data denoising, SVGs identification, trajectory inference, and extraction of 3D spatial domain. By integrating multi-view contrastive graph neural networks, attention mechanisms, and deep iterative clustering, MCGAE achieves precise and customized embeddings, significantly enhancing the reconstruction of spatial structures and the representation of gene expression patterns. This approach adeptly handles the complexities of spatial transcriptomics, providing essential insights into tissue heterogeneity and proving to be of immense value in advanced biomedical research."},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#1-installation","title":"1. Installation","text":""},{"location":"installation/#11-check-the-python-version","title":"1.1 Check the python version","text":"<p>The installation should take a few minutes on a normal computer. To install MCGAE package you must make sure that your python version is over 3.9 If you don\u2019t know the version of python you can check it by:</p> <pre><code>import platform\nplatform.python_version()\n</code></pre> <p>'3.9.16'</p> <p>Note: Because MCGAE pends on pytorch, you should make sure torch is correctly installed.  Now you can install the current release of MCGAE by the following way:</p>"},{"location":"installation/#12-github-repo-clone","title":"1.2  Github repo clone","text":"<p>Download the package from Github and install it locally: recommend conda to install</p> <pre><code>git clone https://github.com/yiwen-yang/MCGAE\ncd MCGAE/\nconda env create -f environment.yml\nconda activate mcgae_env\n</code></pre>"},{"location":"installation/#13-test-success-or-not","title":"1.3 Test success or not","text":"<pre><code>from MCGAE.model import MCGAE\n</code></pre>"},{"location":"references/","title":"Tutorial 2: MCGAE's spatial clustering of colorectal cancer liver metastasis data elucidated the phenomenon of tumor invasion","text":""},{"location":"references/#the-main-references-of-the-article","title":"The main references of the article","text":"<ol> <li>Rao A, Barkley D, Franca GS et al. Exploring tissue architecture using spatial transcriptomics, Nature 2021;596:211-220.</li> <li>Armingol E, Officer A, Harismendy O et al. Deciphering cell-cell interactions and communication from gene expression, Nat Rev Genet 2021;22:71-88.</li> <li>Ji N, van Oudenaarden A. Single molecule fluorescent in situ hybridization (smFISH) of C. elegans worms and embryos, WormBook 2012:1-16.</li> <li>Chen KH, Boettiger AN, Moffitt JR et al. RNA imaging. Spatially resolved, highly multiplexed RNA profiling in single cells, Science 2015;348:aaa6090.</li> <li>Moffitt JR, Bambah-Mukku D, Eichhorn SW et al. Molecular, spatial, and functional single-cell profiling of the hypothalamic preoptic region, Science 2018;362.</li> <li>Lubeck E, Coskun AF, Zhiyentayev T et al. Single-cell in situ RNA profiling by sequential hybridization, Nat Methods 2014;11:360-361.</li> <li>Lee JH, Daugharthy ER, Scheiman J et al. Highly multiplexed subcellular RNA sequencing in situ, Science 2014;343:1360-1363.</li> <li>Wang X, Allen WE, Wright MA et al. Three-dimensional intact-tissue sequencing of single-cell transcriptional states, Science 2018;361.</li> <li>Ji AL, Rubin AJ, Thrane K et al. Multimodal Analysis of Composition and Spatial Architecture in Human Squamous Cell Carcinoma, Cell 2020;182:497-514.e422.</li> <li>Rodriques SG, Stickels RR, Goeva A et al. Slide-seq: A scalable technology for measuring genome-wide expression at high spatial resolution, Science 2019;363:1463-1467.</li> <li>Chen A, Liao S, Cheng M et al. Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays, Cell 2022;185:1777-1792.e1721.</li> <li>Moncada R, Barkley D, Wagner F et al. Integrating microarray-based spatial transcriptomics and single-cell RNA-seq reveals tissue architecture in pancreatic ductal adenocarcinomas, Nat Biotechnol 2020;38:333-342.</li> <li>Satija R, Farrell JA, Gennert D et al. Spatial reconstruction of single-cell gene expression data, Nat Biotechnol 2015;33:495-502.</li> <li>Dries R, Zhu Q, Dong R et al. Giotto: a toolbox for integrative analysis and visualization of spatial expression data, Genome Biol 2021;22:78.</li> <li>Zhao E, Stone MR, Ren X et al. Spatial transcriptomics at subspot resolution with BayesSpace, Nat Biotechnol 2021;39:1375-1384.</li> <li>Long Y, Ang KS, Li M et al. Spatially informed clustering, integration, and deconvolution of spatial transcriptomics with GraphST, Nat Commun 2023;14:1155.</li> <li>Dong K, Zhang S. Deciphering spatial domains from spatially resolved transcriptomics with an adaptive graph attention auto-encoder, Nat Commun 2022;13:1739.</li> <li>Ren H, Walker BL, Cang Z et al. Identifying multicellular spatiotemporal organization of cells with SpaceFlow, Nat Commun 2022;13:4076.</li> <li>Shi X, Zhu J, Long Y et al. Identifying spatial domains of spatially resolved transcriptomics via multi-view graph convolutional networks, Brief Bioinform 2023;24.</li> <li>Hu J, Li X, Coleman K et al. SpaGCN: Integrating gene expression, spatial location and histology to identify spatial domains and spatially variable genes by graph convolutional network, Nat Methods 2021;18:1342-1351.</li> <li>Xu C, Jin X, Wei S et al. DeepST: identifying spatial domains in spatial transcriptomics by deep learning, Nucleic Acids Res 2022;50:e131.</li> <li>Zong Y, Yu T, Wang X et al. conST: an interpretable multi-modal contrastive learning framework for spatial transcriptomics, bioRxiv 2022:2022.2001.2014.476408.</li> <li>Wang B, Luo J, Liu Y et al. Spatial-MGCN: a novel multi-view graph convolutional network for identifying spatial domains with attention mechanism, Briefings in Bioinformatics 2023;24:bbad262.</li> <li>He K, Zhang X, Ren S et al. Deep Residual Learning for Image Recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016, p. 770-778.</li> <li>Zhang C, Yang Y, Tang S et al. Contrastively generative self-expression model for single-cell and spatial multimodal data, Briefings in Bioinformatics 2023;24:bbad265.</li> <li>Kipf TN, Welling M. Semi-Supervised Classification with Graph Convolutional Networks. 2016, arXiv:1609.02907.</li> <li>Veli\u010dkovi\u0107 P, Fedus W, Hamilton WL et al. Deep Graph Infomax. 2018, arXiv:1809.10341.</li> <li>Paszke A, Gross S, Massa F et al. PyTorch: an imperative style, high-performance deep learning library.  Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc., 2019, Article 721.</li> <li>Kingma DP, Ba J. Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980 2014.</li> <li>Paszke A, Gross S, Massa F et al. Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems 2019;32.</li> <li>Maynard KR, Collado-Torres L, Weber LM et al. Transcriptome-scale spatial gene expression in the human dorsolateral prefrontal cortex, Nat Neurosci 2021;24:425-436.</li> <li>Li H, Calder CA, Cressie N. Beyond Moran's I: Testing for Spatial Dependence Based on the Spatial Autoregressive Model, Geographical Analysis 2007;39:357-375.</li> <li>Wu Y, Yang S, Ma J et al. Spatiotemporal Immune Landscape of Colorectal Cancer Liver Metastasis at Single-Cell Level, Cancer Discovery 2022;12:134-153.</li> </ol>"},{"location":"tutorial1/","title":"Tutorial 1: Mouse Olfactory","text":""},{"location":"tutorial1/#load-library","title":"Load library","text":"<pre><code>import os\nimport pandas as pd\nimport scanpy as sc\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport time\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\nfrom sklearn.metrics import adjusted_rand_score as ari_score\nfrom sklearn.cluster import KMeans\nimport itertools\n# Importing custom modules\nfrom MCGAE.model import MCGAE\nfrom MCGAE.utils import load_dataset, norm_and_filter, compute_adata_components, mclust_R, search_res, refine_label, \\\n    set_seed\n</code></pre>"},{"location":"tutorial1/#set-r-kernel-path","title":"Set R kernel path","text":"<pre><code>os.environ['R_HOME'] = r'D:\\Software\\R-4.3.1'\n# the location of R (used for the mclust clustering)\nos.environ['R_USER'] = r'D:\\Software\\Anaconda\\envs\\py3.9\\Lib\\site-packages\\rpy2'\n</code></pre>"},{"location":"tutorial1/#prepare-file-path","title":"Prepare file path","text":"<pre><code>\"\"\"\nBASE_DIR: Project directory\ndata_dir: Data directory\nresult_dir: Result directory\nfile_path: File path\n\"\"\"\nBASE_DIR = r\"D:\\Work\\MCGAE project\\MCGAE-master\"\nfile_path = os.path.join(BASE_DIR, \"benchmark\", \"Mouse_Olafactory\")\ndir_path = os.path.join(file_path, \"raw_data\")\nBASE_DIR = r\"D:\\Work\\MCGAE project\\MCGAE-master\"\nfile_path = os.path.join(BASE_DIR, \"benchmark\", \"Mouse_Olfactory\")\n</code></pre>"},{"location":"tutorial1/#read-data-and-normalization","title":"Read data and normalization","text":"<pre><code># read h5ad\ncounts = pd.read_csv(os.path.join(file_path, \"raw_data\", \"RNA_counts.tsv\"), sep=\"\\t\", index_col=0)\nposition = pd.read_csv(os.path.join(file_path, \"raw_data\", \"position.tsv\"), sep=\"\\t\")\ncounts.columns = ['Spot_' + str(x) for x in counts.columns]\nposition.index = position['label'].map(lambda x: 'Spot_' + str(x))\nposition = position.loc[:, ['x', 'y']]\nadata = sc.AnnData(counts.T)\nadata.var_names_make_unique()\nposition = position.loc[adata.obs_names, [\"y\", \"x\"]]\nadata.obsm[\"spatial\"] = position.to_numpy()\nused_barcode = pd.read_csv(os.path.join(file_path, \"raw_data\", \"used_barcodes.txt\"), sep=\"\\t\", header=None)\nadata = adata[used_barcode[0], :]\nadata.X = torch.from_numpy(adata.X)\nsc.pp.filter_genes(adata, min_cells=50)\nsc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=2000)\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\n</code></pre>"},{"location":"tutorial1/#set-seeds-and-preset-cluster-numbers-for-easy-calculation-of-clustering-loss","title":"Set seeds and preset cluster numbers for easy calculation of clustering loss","text":""},{"location":"tutorial1/#build-multiple-view-inputs-simultaneously","title":"Build multiple view inputs simultaneously","text":"<pre><code>set_seed(1234)\ncompute_adata_components(adata, n_components=100)\nn_clusters = 7\nsave_obj_z = pd.DataFrame()\n</code></pre> <pre><code>Graph constructed!\nFor denoise data training: Epoch [90/100], Loss: 0.0009\nGraph_hat constructed!\n</code></pre>"},{"location":"tutorial1/#train-model","title":"Train model","text":"<pre><code>begin = time.time()\nmodel = MCGAE(\n    adata,\n    n_latent=50,\n    n_components=100,\n    use_pca=True,\n    fusion_mode=\"fractional\",\n    use_emb_x_rec=True,\n    use_emb_g_rec=True,\n    dropout=0.01,\n    random_seed=20,\n)\n\nmodel.train(\n    max_epochs=600, \n    weight_decay=5e-4,\n    w_recon_x=0.05,\n    w_recon_g=0.01,\n    w_contrast=0.01,\n    w_cluster=0.1,\n    n_clusters=n_clusters,\n    cl_start_epoch=100,\n    compute_g_loss=\"cross_entropy\",\n)\n</code></pre> <pre><code>Searching resolution...\nInitializing cluster centers with leiden, resolution =  0.9000000000000002\n\ntraining:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 441/600 [32:21&lt;10:29,  3.96s/it]\n\nEpoch: 440, Loss: 0.327582985162735\n\ndelta_label 0.0 &lt; tol 1e-06\nReach tolerance threshold. Stopping training.\nTotal epoch: 449\n</code></pre>"},{"location":"tutorial1/#the-representation-of-the-model-after-training-is-obtained","title":"The representation of the model after training is obtained","text":"<pre><code>output_mod = model.get_model_output()\nemb, y_pred = output_mod[\"emb\"], output_mod[\"y_pred\"]\nadata.obsm[\"z\"] = emb\n</code></pre> <pre><code>res = search_res(adata, n_clusters, rep=\"z\", start=0.05, end=1)\nsc.pp.neighbors(adata, use_rep='z', random_state=1234)\nsc.tl.umap(adata)\nsc.tl.leiden(adata, resolution=res, random_state=1234)\n</code></pre> <pre><code>Searching resolution...\nresolution=0.15000000000000002, cluster number=7\n</code></pre> <pre><code>plt.rcParams[\"figure.figsize\"] = (5, 5)\nsc.pl.embedding(adata, basis=\"spatial\", color=\"leiden\", s=20, show=False, title='MCGAE')\n</code></pre> <pre><code>&lt;Axes: title={'center': 'MCGAE'}, xlabel='spatial1', ylabel='spatial2'&gt;\n</code></pre>"},{"location":"tutorial1/#tutorial-1-mouse-olfactory_1","title":"Tutorial 1: Mouse Olfactory","text":""},{"location":"tutorial1/#load-library_1","title":"Load library","text":"<pre><code>import os\nimport pandas as pd\nimport scanpy as sc\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport time\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\nfrom sklearn.metrics import adjusted_rand_score as ari_score\nfrom sklearn.cluster import KMeans\nimport itertools\n# Importing custom modules\nfrom MCGAE.model import MCGAE\nfrom MCGAE.utils import load_dataset, norm_and_filter, compute_adata_components, mclust_R, search_res, refine_label, \\\n    set_seed\n</code></pre>"},{"location":"tutorial1/#set-r-kernel-path_1","title":"Set R kernel path","text":"<pre><code>os.environ['R_HOME'] = r'D:\\Software\\R-4.3.1'\n# the location of R (used for the mclust clustering)\nos.environ['R_USER'] = r'D:\\Software\\Anaconda\\envs\\py3.9\\Lib\\site-packages\\rpy2'\n</code></pre>"},{"location":"tutorial1/#prepare-file-path_1","title":"Prepare file path","text":"<pre><code>\"\"\"\nBASE_DIR: Project directory\ndata_dir: Data directory\nresult_dir: Result directory\nfile_path: File path\n\"\"\"\nBASE_DIR = r\"D:\\Work\\MCGAE project\\MCGAE-master\"\nfile_path = os.path.join(BASE_DIR, \"benchmark\", \"Mouse_Olafactory\")\ndir_path = os.path.join(file_path, \"raw_data\")\nBASE_DIR = r\"D:\\Work\\MCGAE project\\MCGAE-master\"\nfile_path = os.path.join(BASE_DIR, \"benchmark\", \"Mouse_Olfactory\")\n</code></pre>"},{"location":"tutorial1/#read-data-and-normalization_1","title":"Read data and normalization","text":"<pre><code># read h5ad\ncounts = pd.read_csv(os.path.join(file_path, \"raw_data\", \"RNA_counts.tsv\"), sep=\"\\t\", index_col=0)\nposition = pd.read_csv(os.path.join(file_path, \"raw_data\", \"position.tsv\"), sep=\"\\t\")\ncounts.columns = ['Spot_' + str(x) for x in counts.columns]\nposition.index = position['label'].map(lambda x: 'Spot_' + str(x))\nposition = position.loc[:, ['x', 'y']]\nadata = sc.AnnData(counts.T)\nadata.var_names_make_unique()\nposition = position.loc[adata.obs_names, [\"y\", \"x\"]]\nadata.obsm[\"spatial\"] = position.to_numpy()\nused_barcode = pd.read_csv(os.path.join(file_path, \"raw_data\", \"used_barcodes.txt\"), sep=\"\\t\", header=None)\nadata = adata[used_barcode[0], :]\nadata.X = torch.from_numpy(adata.X)\nsc.pp.filter_genes(adata, min_cells=50)\nsc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=2000)\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\n</code></pre>"},{"location":"tutorial1/#set-seeds-and-preset-cluster-numbers-for-easy-calculation-of-clustering-loss_1","title":"Set seeds and preset cluster numbers for easy calculation of clustering loss","text":""},{"location":"tutorial1/#build-multiple-view-inputs-simultaneously_1","title":"Build multiple view inputs simultaneously","text":"<pre><code>set_seed(1234)\ncompute_adata_components(adata, n_components=100)\nn_clusters = 7\nsave_obj_z = pd.DataFrame()\n</code></pre> <pre><code>Graph constructed!\nFor denoise data training: Epoch [90/100], Loss: 0.0009\nGraph_hat constructed!\n</code></pre>"},{"location":"tutorial1/#train-model_1","title":"Train model","text":"<pre><code>begin = time.time()\nmodel = MCGAE(\n    adata,\n    n_latent=50,\n    n_components=100,\n    use_pca=True,\n    fusion_mode=\"fractional\",\n    use_emb_x_rec=True,\n    use_emb_g_rec=True,\n    dropout=0.01,\n    random_seed=20,\n)\n\nmodel.train(\n    max_epochs=600, \n    weight_decay=5e-4,\n    w_recon_x=0.05,\n    w_recon_g=0.01,\n    w_contrast=0.01,\n    w_cluster=0.1,\n    n_clusters=n_clusters,\n    cl_start_epoch=100,\n    compute_g_loss=\"cross_entropy\",\n)\n</code></pre> <pre><code>Searching resolution...\nInitializing cluster centers with leiden, resolution =  0.9000000000000002\n\ntraining:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 441/600 [32:21&lt;10:29,  3.96s/it]\n\nEpoch: 440, Loss: 0.327582985162735\n\ndelta_label 0.0 &lt; tol 1e-06\nReach tolerance threshold. Stopping training.\nTotal epoch: 449\n</code></pre>"},{"location":"tutorial1/#the-representation-of-the-model-after-training-is-obtained_1","title":"The representation of the model after training is obtained","text":"<pre><code>output_mod = model.get_model_output()\nemb, y_pred = output_mod[\"emb\"], output_mod[\"y_pred\"]\nadata.obsm[\"z\"] = emb\n</code></pre> <pre><code>res = search_res(adata, n_clusters, rep=\"z\", start=0.05, end=1)\nsc.pp.neighbors(adata, use_rep='z', random_state=1234)\nsc.tl.umap(adata)\nsc.tl.leiden(adata, resolution=res, random_state=1234)\n</code></pre> <pre><code>Searching resolution...\nresolution=0.15000000000000002, cluster number=7\n</code></pre> <pre><code>plt.rcParams[\"figure.figsize\"] = (5, 5)\nsc.pl.embedding(adata, basis=\"spatial\", color=\"leiden\", s=20, show=False, title='MCGAE')\n</code></pre> <pre><code>&lt;Axes: title={'center': 'MCGAE'}, xlabel='spatial1', ylabel='spatial2'&gt;\n</code></pre>"},{"location":"tutorial2/","title":"Tutorial 2: MCGAE constructs 3D spatial domains to mitigate batch effects","text":""},{"location":"tutorial2/#load-library","title":"Load library","text":"<pre><code>import os\nimport pandas as pd\nimport scanpy as sc\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport time\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\nfrom sklearn.metrics import adjusted_rand_score as ari_score\nfrom sklearn.cluster import KMeans\nimport itertools\nfrom MCGAE.model import MCGAE\nfrom MCGAE.utils import load_dataset, norm_and_filter, compute_adata_components, mclust_R, search_res, refine_label, \\\n    set_seed\nfrom MCGAE.utils import Cal_Spatial_Net_3D\n\n</code></pre>"},{"location":"tutorial2/#set-r-kernel-path","title":"Set R kernel path","text":"<pre><code>os.environ['R_HOME'] = r'D:\\Software\\R-4.3.1'\n# the location of R (used for the mclust clustering)\nos.environ['R_USER'] = r'D:\\Software\\Anaconda\\envs\\py3.9\\Lib\\site-packages\\rpy2'\n</code></pre>"},{"location":"tutorial2/#prepare-file-path","title":"Prepare file path","text":"<pre><code>\"\"\"\nBASE_DIR: Project directory\ndata_dir: Data directory\nresult_dir: Result directory\nfile_path: File path\n\"\"\"\nBASE_DIR = r\"D:\\Work\\MCGAE project\\MCGAE-master\"\nfile_path = os.path.join(BASE_DIR, \"benchmark\", \"3D hippocampus\")\n\ncounts = pd.read_csv(os.path.join(file_path, \"raw_data\", \"3D_Hippo_expression.txt\"), sep=\"\\t\",\n                     index_col=0)\nposition = pd.read_csv(os.path.join(file_path, \"raw_data\", \"ICP_Align_Coor.txt\"), sep=\"\\t\", index_col=0)\nadata = sc.AnnData(counts)\nsc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=3000)\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\n</code></pre>"},{"location":"tutorial2/#read-data-and-normalization","title":"read data and normalization","text":"<pre><code># loading metadata and aligned coordinates\nadata.obs['X'] = position.loc[adata.obs_names, 'X']\nadata.obs['Y'] = position.loc[adata.obs_names, 'Y']\nadata.obs['Z'] = position.loc[adata.obs_names, 'Z']\nadata.obs['Section_id'] = position.loc[adata.obs_names, 'Section']\n</code></pre>"},{"location":"tutorial2/#provide-color-for-each-sample","title":"Provide color for each sample","text":"<pre><code># loading the spatial locations\nadata.obsm['spatial'] = adata.obs.loc[:, ['X', 'Y']].values\nsection_colors = ['#02899A', '#0E994D', '#86C049', '#FBB21F', '#F48022', '#DA5326', '#BA3326']\n</code></pre>"},{"location":"tutorial2/#set-up-the-canvas-and-view-the-initial-sample-distribution","title":"Set up the canvas and view the initial sample distribution","text":"<pre><code>fig = plt.figure(figsize=(4, 4))\nax1 = plt.axes(projection='3d')\nfor it, label in enumerate(np.unique(adata.obs['Section_id'])):\n    temp_Coor = adata.obs.loc[adata.obs['Section_id'] == label, :]\n    temp_xd = temp_Coor['X']\n    temp_yd = temp_Coor['Y']\n    temp_zd = temp_Coor['Z']\n    ax1.scatter3D(temp_xd, temp_yd, temp_zd, c=section_colors[it], s=0.2, marker=\"o\", label=label)\n\nax1.set_xlabel('')\nax1.set_ylabel('')\nax1.set_zlabel('')\n\nax1.set_xticklabels([])\nax1.set_yticklabels([])\nax1.set_zticklabels([])\n\nplt.legend(bbox_to_anchor=(1, 0.8), markerscale=10, frameon=False)\n\nax1.elev = 45\nax1.azim = -20\n\nplt.show()\n</code></pre>"},{"location":"tutorial2/#computes-multiple-view-components-and-calculates-neighbor-relationships-between-adjacent-samples","title":"Computes multiple view components and calculates neighbor relationships between adjacent samples","text":"<pre><code>set_seed(1234)\ncompute_adata_components(adata, n_components=100)\n# construct 3D\nCal_Spatial_Net_3D(adata, key_section='Section_id', num=32, p=0.2)\nadata.obsm[\"adj_orig\"] = adata.obsm[\"combined_adjacency_KNN\"]\nadata.obsm[\"adj_aug\"] = adata.obsm[\"combined_adjacency_cosine\"]\nadata.obsm[\"graph_orig\"] = adata.obsm[\"graph_knn\"]\n\n</code></pre> <pre><code>Graph constructed!\nFor denoise data training: Epoch [0/100], Loss: 0.0018\nFor denoise data training: Epoch [10/100], Loss: 0.0009\nFor denoise data training: Epoch [20/100], Loss: 0.0009\nFor denoise data training: Epoch [30/100], Loss: 0.0009\nFor denoise data training: Epoch [40/100], Loss: 0.0009\nFor denoise data training: Epoch [50/100], Loss: 0.0009\nFor denoise data training: Epoch [60/100], Loss: 0.0009\nFor denoise data training: Epoch [70/100], Loss: 0.0009\nFor denoise data training: Epoch [80/100], Loss: 0.0009\nFor denoise data training: Epoch [90/100], Loss: 0.0009\nGraph_hat constructed!\nRun 1: l [0.01, 1000], p [0.0, 2687.040892295065]\nRun 2: l [0.01, 500.005], p [0.0, 1327.6409912109375]\nRun 3: l [0.01, 250.0075], p [0.0, 569.8638916015625]\nRun 4: l [0.01, 125.00874999999999], p [0.0, 223.31988525390625]\nRun 5: l [0.01, 62.509375], p [0.0, 72.82575225830078]\nRun 6: l [0.01, 31.2596875], p [0.0, 20.441387176513672]\nRun 7: l [0.01, 15.63484375], p [0.0, 5.05623722076416]\nRun 8: l [0.01, 7.822421875], p [0.0, 0.9642606973648071]\nRun 9: l [3.9162109375, 7.822421875], p [0.17983782291412354, 0.9642606973648071]\nRun 10: l [3.9162109375, 5.86931640625], p [0.17983782291412354, 0.44789183139801025]\nRun 11: l [3.9162109375, 4.8927636718750005], p [0.17983782291412354, 0.28901946544647217]\nRun 12: l [3.9162109375, 4.4044873046875], p [0.17983782291412354, 0.22960197925567627]\nrecommended l =  4.16034912109375\ncombined_adjacency_gaussian constructed!\ncombined_adjacency_KNN constructed!\ncombined_adjacency_cosine constructed!\nmulti view Combined adjacency matrix constructed!\n</code></pre>"},{"location":"tutorial2/#train-model","title":"Train model","text":"<pre><code>model = MCGAE(\n        adata,\n        n_latent=50,\n        n_components=100,\n        use_pca=True,\n        fusion_mode=\"fractional\",\n        use_emb_x_rec=True,\n        use_emb_g_rec=True,\n        dropout=0.01,\n        random_seed=8,\n    )\n\nmodel.train(weight_decay=5e-4,\n                max_epochs=600,\n                w_recon_x=0.05,\n                w_recon_g=0.01,\n                w_contrast=0.01,\n                w_cluster=0.1,\n                cl_start_epoch=100,\n                n_clusters=4,\n                compute_g_loss=\"cross_entropy\",\n                adj_diag=1,\n                cluster_method=\"kmeans\",\n                )\n</code></pre> <pre><code>Initializing cluster centers with kmeans, n_clusters known\n\n\ntraining:   0%|\u258f                                                                       | 2/600 [00:00&lt;01:44,  5.72it/s]\n\nEpoch: 0, Loss: 0.46930643916130066\n\n\ntraining:   4%|\u2588\u2588\u258c                                                                    | 22/600 [00:03&lt;01:30,  6.37it/s]\n\nEpoch: 20, Loss: 0.4749948978424072\n\n\ntraining:   7%|\u2588\u2588\u2588\u2588\u2589                                                                  | 42/600 [00:06&lt;01:20,  6.94it/s]\n\nEpoch: 40, Loss: 0.4679427742958069\n\n\ntraining:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                               | 62/600 [00:09&lt;01:16,  7.04it/s]\n\nEpoch: 60, Loss: 0.45564910769462585\n\n\ntraining:  14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                             | 82/600 [00:12&lt;01:13,  7.01it/s]\n\nEpoch: 80, Loss: 0.4409998059272766\n\n\ntraining:  17%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                          | 102/600 [00:14&lt;01:10,  7.04it/s]\n\nEpoch: 100, Loss: 0.43004512786865234\n\n\ntraining:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                       | 122/600 [00:17&lt;01:08,  7.00it/s]\n\nEpoch: 120, Loss: 0.42057886719703674\n\n\ntraining:  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                     | 142/600 [00:20&lt;01:06,  6.93it/s]\n\nEpoch: 140, Loss: 0.41338834166526794\n\n\ntraining:  27%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                   | 162/600 [00:23&lt;01:03,  6.88it/s]\n\nEpoch: 160, Loss: 0.4078400135040283\n\n\ntraining:  30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                | 182/600 [00:26&lt;01:00,  6.91it/s]\n\nEpoch: 180, Loss: 0.40357333421707153\n\n\ntraining:  34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                              | 202/600 [00:29&lt;00:58,  6.78it/s]\n\nEpoch: 200, Loss: 0.4002784788608551\n\n\ntraining:  37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                            | 222/600 [00:32&lt;00:55,  6.78it/s]\n\nEpoch: 220, Loss: 0.3975831866264343\n\n\ntraining:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                         | 242/600 [00:35&lt;00:50,  7.07it/s]\n\nEpoch: 240, Loss: 0.3953728675842285\n\n\ntraining:  44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                       | 262/600 [00:38&lt;00:47,  7.06it/s]\n\nEpoch: 260, Loss: 0.39355236291885376\n\n\ntraining:  47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                     | 282/600 [00:41&lt;00:45,  7.05it/s]\n\nEpoch: 280, Loss: 0.39198607206344604\n\n\ntraining:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                  | 302/600 [00:43&lt;00:42,  6.95it/s]\n\nEpoch: 300, Loss: 0.39062654972076416\n\n\ntraining:  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                | 322/600 [00:46&lt;00:43,  6.40it/s]\n\nEpoch: 320, Loss: 0.38944587111473083\n\n\ntraining:  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                              | 342/600 [00:49&lt;00:39,  6.61it/s]\n\nEpoch: 340, Loss: 0.3884800672531128\n\n\ntraining:  60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                           | 362/600 [00:52&lt;00:34,  6.85it/s]\n\nEpoch: 360, Loss: 0.3876018226146698\n\n\ntraining:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                         | 382/600 [00:55&lt;00:31,  6.83it/s]\n\nEpoch: 380, Loss: 0.38675832748413086\n\n\ntraining:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                       | 402/600 [00:58&lt;00:28,  6.90it/s]\n\nEpoch: 400, Loss: 0.3859403133392334\n\n\ntraining:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                       | 402/600 [00:58&lt;00:28,  6.85it/s]\n\ndelta_label 0.0 &lt; tol 1e-06\nReach tolerance threshold. Stopping training.\nTotal epoch: 402\n</code></pre>"},{"location":"tutorial2/#analysis-of-model-calculation-results","title":"Analysis of model calculation results","text":"<pre><code>    temp = model.get_model_output()\n    emb, y_pred, emb_rec = temp[\"emb\"], temp[\"y_pred\"], temp[\"x_rec\"]\n    adata.obsm[\"z\"] = emb\n    adata.obs[\"y_pred\"] = y_pred\n    sc.pp.neighbors(adata, use_rep=\"z\")\n    sc.tl.umap(adata)\n    num_cluster = 4\n    adata = mclust_R(adata, num_cluster, used_obsm='z')\n    adata.obs['mclust'] = adata.obs['mclust'].astype('str')\n    # sc.tl.leiden(adata, resolution=0.05)\n    plt.rcParams[\"figure.figsize\"] = (3, 3)\n    sc.pl.umap(adata, color=['mclust', 'Section_id'], show=False)\n    fig = plt.figure(figsize=(4, 4))\n    ax1 = plt.axes(projection='3d')\n    for it, label in enumerate(np.unique(adata.obs['mclust'])):\n        temp_Coor = adata.obs.loc[adata.obs['mclust'] == label, :]\n        temp_xd = temp_Coor['X']\n        temp_yd = temp_Coor['Y']\n        temp_zd = temp_Coor['Z']\n        ax1.scatter3D(temp_xd, temp_yd, temp_zd, c=adata.uns['mclust_colors'][it], s=30, marker=\"o\", label=label)\n\n    ax1.set_xlabel('')\n    ax1.set_ylabel('')\n    ax1.set_zlabel('')\n\n    ax1.set_xticklabels([])\n    ax1.set_yticklabels([])\n    ax1.set_zticklabels([])\n\n    plt.legend(bbox_to_anchor=(1.2, 0.8), markerscale=1, frameon=False)\n    plt.title('MCGAE-3D')\n    ax1.view_init(elev=0, azim=0)\n    ax1.elev = 45\n    ax1.azim = -20\n    plt.show()\n</code></pre> <pre><code>R[write to console]:                    __           __ \n   ____ ___  _____/ /_  _______/ /_\n  / __ `__ \\/ ___/ / / / / ___/ __/\n / / / / / / /__/ / /_/ (__  ) /_  \n/_/ /_/ /_/\\___/_/\\__,_/____/\\__/   version 6.0.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\n\n\nfitting ...\n  |======================================================================| 100%\n</code></pre>"},{"location":"tutorial3/","title":"Tutorial 2: MCGAE's spatial clustering of colorectal cancer liver metastasis data elucidated the phenomenon of tumor invasion","text":""},{"location":"tutorial3/#load-library","title":"Load library","text":"<pre><code>import os\nimport pandas as pd\nimport scanpy as sc\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\nfrom sklearn.metrics import adjusted_rand_score as ari_score\nfrom sklearn.cluster import KMeans\nimport itertools\nfrom MCGAE.model import MCGAE\nfrom MCGAE.utils import load_dataset, norm_and_filter, compute_adata_components, search_res, refine_label, set_seed, \\\n    Moran_I\n\n# Suppressing runtime warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n</code></pre>"},{"location":"tutorial3/#prepare-file-path","title":"Prepare file path","text":"<pre><code>\"\"\"\nBASE_DIR: Project directory\ndata_dir: Data directory\nresult_dir: Result directory\nfile_path: File path\n\"\"\"\nfile_name = \"ST-colon1\"\n\nBASE_DIR = r\"D:\\Work\\MCGAE project\\MCGAE-master\"\nfile_path = os.path.join(BASE_DIR, \"benchmark\", \"Colorectal Cancer\", f\"{file_name}\")\ndir_path = os.path.join(file_path, \"raw_data\")\n</code></pre>"},{"location":"tutorial3/#extract-image-information","title":"Extract image information","text":"<pre><code>adata = load_dataset(dir_path, use_image=True)\nsc.pp.filter_genes(adata, min_cells=1)\nsc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=3000)\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\nadata = adata[:, adata.var[\"highly_variable\"]]\nn_clusters = 20\nprint(n_clusters)\n</code></pre> <pre><code>Tiling image: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 [ time left: 00:00 ]\nExtract feature: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 [ time left: 00:00 ]\n\n\nThe morphology feature is added to adata.obsm['X_morphology']!\n20\nGraph constructed!\n\n\nadata.obsm[\"graph_orig\"] = interaction\n</code></pre>"},{"location":"tutorial3/#compute-multiple-view-components","title":"Compute multiple view components","text":"<pre><code>set_seed(1234)\ncompute_adata_components(adata, n_components=100)\nsave_obj_z = pd.DataFrame()\n</code></pre> <pre><code>For denoise data training: Epoch [0/100], Loss: 0.0041\nFor denoise data training: Epoch [10/100], Loss: 0.0006\nFor denoise data training: Epoch [20/100], Loss: 0.0006\nFor denoise data training: Epoch [30/100], Loss: 0.0006\nFor denoise data training: Epoch [40/100], Loss: 0.0006\nFor denoise data training: Epoch [50/100], Loss: 0.0006\nFor denoise data training: Epoch [60/100], Loss: 0.0006\nFor denoise data training: Epoch [70/100], Loss: 0.0006\nFor denoise data training: Epoch [80/100], Loss: 0.0006\nFor denoise data training: Epoch [90/100], Loss: 0.0006\nGraph_hat constructed!\n</code></pre>"},{"location":"tutorial3/#train-model","title":"Train model","text":"<pre><code>model = MCGAE(\n    adata,\n    n_latent=50,\n    n_components=100,\n    use_pca=True,\n    # fusion_mode=\"holistic\",\n    fusion_mode=\"fractional\",\n    # fusion_mode=\"vanilla\",\n    use_emb_x_rec=True,\n    use_emb_g_rec=True,\n    dropout=0.01,  # 0.01\n    random_seed=12,\n    w_morph=1.9,\n)\n\nmodel.train(\n    weight_decay=5e-4,\n    # weight_decay=0.0,\n    w_recon_x=0.05,\n    w_recon_g=0.1,\n    w_contrast=0.1,\n    w_cluster=0.1,\n    n_clusters=n_clusters,\n    cl_start_epoch=100,\n    compute_g_loss=\"cross_entropy\",\n)\n</code></pre> <pre><code>Now the cycle is: 12\nSearching resolution...\n\ntraining:   2%|\u2588\u258e                                                                      | 7/400 [00:00&lt;00:17, 22.04it/s]\n\nEpoch: 0, Loss: 2.7255704402923584\n\n\ntraining:   8%|\u2588\u2588\u2588\u2588\u2588\u258c                                                                 | 31/400 [00:00&lt;00:07, 50.33it/s]\n\nEpoch: 20, Loss: 1.2939739227294922\n\n\ntraining:  12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                              | 50/400 [00:01&lt;00:06, 55.35it/s]\n\nEpoch: 40, Loss: 0.9415194988250732\n\n\ntraining:  17%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                          | 69/400 [00:01&lt;00:05, 56.74it/s]\n\nEpoch: 60, Loss: 0.7995103001594543\n\n\ntraining:  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                       | 87/400 [00:01&lt;00:05, 57.74it/s]\n\nEpoch: 80, Loss: 0.7482431530952454\n\n\ntraining:  28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                  | 112/400 [00:02&lt;00:04, 58.68it/s]\n\nEpoch: 100, Loss: 0.7114622592926025\n\n\ntraining:  32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                               | 130/400 [00:02&lt;00:04, 58.09it/s]\n\nEpoch: 120, Loss: 0.6843234300613403\n\n\ntraining:  37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                            | 149/400 [00:02&lt;00:04, 58.12it/s]\n\nEpoch: 140, Loss: 0.6560841202735901\n\n\ntraining:  43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                       | 173/400 [00:03&lt;00:03, 57.56it/s]\n\nEpoch: 160, Loss: 0.6310422420501709\n\n\ntraining:  48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                    | 191/400 [00:03&lt;00:03, 57.97it/s]\n\nEpoch: 180, Loss: 0.610443115234375\n\n\ntraining:  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                 | 209/400 [00:03&lt;00:03, 58.08it/s]\n\nEpoch: 200, Loss: 0.593916654586792\n\n\ntraining:  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                              | 227/400 [00:04&lt;00:02, 57.91it/s]\n\nEpoch: 220, Loss: 0.5803815722465515\n\n\ntraining:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                          | 251/400 [00:04&lt;00:02, 58.28it/s]\n\nEpoch: 240, Loss: 0.5691173076629639\n\n\ntraining:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                       | 269/400 [00:04&lt;00:02, 58.02it/s]\n\nEpoch: 260, Loss: 0.560197114944458\n\n\ntraining:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                  | 293/400 [00:05&lt;00:01, 57.74it/s]\n\nEpoch: 280, Loss: 0.5517188310623169\n\n\ntraining:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d               | 311/400 [00:05&lt;00:01, 58.08it/s]\n\nEpoch: 300, Loss: 0.5455691814422607\n\n\ntraining:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a            | 330/400 [00:05&lt;00:01, 58.34it/s]\n\nEpoch: 320, Loss: 0.5398231744766235\n\n\ntraining:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589         | 348/400 [00:06&lt;00:00, 57.37it/s]\n\nEpoch: 340, Loss: 0.5346991419792175\n\n\ntraining:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     | 372/400 [00:06&lt;00:00, 58.27it/s]\n\nEpoch: 360, Loss: 0.5296562314033508\n\n\ntraining:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 390/400 [00:07&lt;00:00, 58.23it/s]\n\nEpoch: 380, Loss: 0.5251146554946899\n\n\ntraining: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:07&lt;00:00, 55.49it/s]\n\nEpoch: 399, Loss: 0.5210220813751221\n</code></pre>"},{"location":"tutorial3/#analysis-of-model-calculation-results","title":"Analysis of model calculation results","text":"<pre><code>temp = model.get_model_output()\nemb, y_pred, emb_rec = temp[\"emb\"], temp[\"y_pred\"], temp[\"x_rec\"]\nadata.obsm[\"z\"] = emb\nadata.obs[\"pred\"] = y_pred\nres = search_res(adata, n_clusters, rep=\"z\", start=0.3, end=3, increment=0.02)\nsc.pp.neighbors(adata, use_rep=\"z\", n_neighbors=10, random_state=1234)\nsc.tl.leiden(adata, key_added=\"leiden\", resolution=res, random_state=1234)\nnew_type = refine_label(adata, key='leiden', radius=30)\nadata.obs['leiden'] = new_type\nsc.pl.spatial(adata, img_key=\"hires\", color='leiden', title=\"MCGAE\")\nplt.tight_layout()\n</code></pre> <pre><code>Searching resolution...\n</code></pre> <pre><code>&lt;Figure size 1920x1440 with 0 Axes&gt;\n</code></pre>"},{"location":"tutorial3/#data-expression-after-noise-reduction","title":"Data expression after noise reduction","text":"<pre><code>sc.pp.scale(adata, zero_center=False, max_value=1)\nadata2 = adata.copy()\nadata2.X = emb_rec\nsc.pl.spatial(adata, img_key=\"hires\", color=\"TF\", title=\"TF_raw_Exp\", color_map=\"RdYlGn_r\")\nsc.pl.spatial(adata2, img_key=\"hires\", color=\"TF\", title=\"TF_denoise\", color_map=\"RdYlGn_r\")\nsc.pl.spatial(adata, img_key=\"hires\", color=\"ALB\", title=\"ALB_raw_Exp\", color_map=\"RdYlGn_r\")\nsc.pl.spatial(adata2, img_key=\"hires\", color=\"ALB\", title=\"ALB_denoise\", color_map=\"RdYlGn_r\")\n</code></pre>"}]}